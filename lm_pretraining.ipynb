{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "from nvstatsrecorder.callbacks import NVStats, NVLinkStats\n",
    "from xfmers import models\n",
    "from xfmers import utils\n",
    "from xfmers import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n",
      "Batch size: 640\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 160\n",
    "BUFFER_SIZE = 10000000\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 12\n",
    "D_MODEL = NUM_HEADS * 64\n",
    "FF_UNITS = D_MODEL * 4\n",
    "DROPOUT = 0.1\n",
    "WEIGHT_SHARING = False\n",
    "EFFICIENT_ATTENTION = False\n",
    "SHARED_QK = False\n",
    "CONV_FILTER = 1\n",
    "CONV_PADDING = \"same\"\n",
    "ACTIVATION = ops.mish\n",
    "REVERSIBLE = False\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "BATCH_SIZE *= strategy.num_replicas_in_sync\n",
    "print(\"Batch size:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character set: [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 30\n",
      "Vocab size padded to: 32 (performance reasons)\n"
     ]
    }
   ],
   "source": [
    "def load_text8(text8_path, num_char=None):\n",
    "    with open(text8_path, 'r', encoding=\"utf8\") as file:\n",
    "        data = file.read().replace('\\n', '').strip()\n",
    "    if num_char:\n",
    "        data = data[:num_char]\n",
    "    return data.strip()\n",
    "\n",
    "text_8_train = load_text8(\"./data/train.txt.raw\")\n",
    "char_set = list(set(list(text_8_train)))\n",
    "char_set.sort()\n",
    "print(\"Character set:\", char_set)\n",
    "len_corpus = len(text_8_train)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=len(char_set)+1, char_level=True)\n",
    "tokenizer.fit_on_texts([\"\".join(char_set)])\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.num_words], [tokenizer.num_words + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.num_words + 2\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n",
    "\n",
    "# pad to 8\n",
    "vocab_size_mult = VOCAB_SIZE // 8\n",
    "vocab_size_r = VOCAB_SIZE % 8\n",
    "if vocab_size_r > 0:\n",
    "    VOCAB_SIZE = (vocab_size_mult + 1) * 8\n",
    "    print(\"Vocab size padded to:\", VOCAB_SIZE, \"(performance reasons)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 256/5624992 [00:00<2:32:06, 616.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Token count: 89999999\n",
      "Generating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5624992/5624992 [00:48<00:00, 115442.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training sequences: (5624992, 129)\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(corpus, tokenizer, seq_len=512):\n",
    "    \"\"\"Tokenize, filter and pad sentences\"\"\"\n",
    "    print(\"Encoding corpus...\")\n",
    "    corpus = tokenizer.texts_to_sequences([corpus])[0]\n",
    "    total_words = len(corpus)\n",
    "    print(\"Done! Token count:\", total_words)\n",
    "    print(\"Generating sequences...\")\n",
    "    list_seq = []\n",
    "    for i in trange(0, total_words-seq_len, seq_len//8):\n",
    "        seq = START_TOKEN + corpus[i:i+seq_len-2] + END_TOKEN + [corpus[i+seq_len-1]]\n",
    "        list_seq.append(seq)\n",
    "    print(\"Done!\")\n",
    "    return np.asarray(list_seq, dtype=\"int\")\n",
    "\n",
    "list_seq = prepare_sequences(text_8_train, tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "print(\"Training sequences:\", list_seq.shape)\n",
    "\n",
    "del text_8_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 44725/312492 [00:00<00:01, 184252.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Token count: 4999999\n",
      "Generating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312492/312492 [00:01<00:00, 277879.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Validation sequences: (312492, 129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_8_val = load_text8(\"./data/valid.txt.raw\")\n",
    "\n",
    "len_corpus = len(text_8_val)\n",
    "\n",
    "list_seq_val = prepare_sequences(text_8_val, tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "del text_8_val\n",
    "\n",
    "print(\"Validation sequences:\", list_seq_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'inputs': list_seq[:,:-1]},\n",
    "                                              {'outputs': list_seq[:,-1]},))\n",
    "dataset = dataset.repeat(-1)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(32)\n",
    "_ = dataset.take(1)\n",
    "\n",
    "# validation\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'inputs': list_seq_val[:,:-1]},\n",
    "                                                  {'outputs': list_seq_val[:,-1]},))\n",
    "val_dataset = val_dataset.repeat(-1)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE*2)\n",
    "val_dataset = val_dataset.prefetch(32)\n",
    "_ = val_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = models.DecoderTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        dec_layers=NUM_LAYERS,\n",
    "        ff_units=FF_UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        weight_sharing=WEIGHT_SHARING,\n",
    "        efficient_attention=EFFICIENT_ATTENTION,\n",
    "        shared_qk=SHARED_QK,\n",
    "        activation=ACTIVATION,\n",
    "        reversible=REVERSIBLE,\n",
    "    )\n",
    "    \n",
    "    learning_rate = utils.NoamSchedule(D_MODEL)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate)\n",
    "    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[utils.bpc])\n",
    "    model.run_eagerly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DecoderTransformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PaddingMaskGenerator (PaddingMa (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "TokenPosEmbedding (TokenPosEmbe (None, None, 768)    122880      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "DecoderBlock (TransformerStack) (None, None, 768)    28351488    PaddingMaskGenerator[0][0]       \n",
      "                                                                 TokenPosEmbedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "outputs (LMHead)                (None, 32)           24608       DecoderBlock[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 28,498,976\n",
      "Trainable params: 28,498,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_stats = NVStats(gpu_index=0, interval=1)\n",
    "SUDO_PASSWORD = \"volta\"\n",
    "nvlink_stats = NVLinkStats(SUDO_PASSWORD, gpus=[0,1,2,3])\n",
    "time_history = utils.TimeHistory()\n",
    "callbacks = [\n",
    "    #tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}-{loss:.2f}.h5', save_weights_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs', write_graph=True, update_freq=1000, profile_batch=10),\n",
    "    time_history,\n",
    "    nv_stats,\n",
    "    nvlink_stats\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = int(list_seq.shape[0]/BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "    model.fit(dataset, epochs=10, steps_per_epoch=train_steps,\n",
    "              validation_data=val_dataset, validation_steps=100,\n",
    "              verbose=1, callbacks=callbacks)\n",
    "    \n",
    "epoch_time = min(time_history.times)\n",
    "tokens_per_epoch = BATCH_SIZE*train_steps*MAX_SEQ_LEN\n",
    "print(\"Tokens/sec:\", int(tokens_per_epoch/epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_stats_recorder = nv_stats.recorder\n",
    "nvlink_stats_recorder = nvlink_stats.recorder\n",
    "nv_stats_recorder.plot_gpu_util(smooth=5)\n",
    "nvlink_stats_recorder.plot_nvlink_traffic(smooth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
